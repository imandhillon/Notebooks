Gaussian Mixture Models


To understand Gaussian Mixture Models (GMMs), we need to understand what mixture models are.
 - A probabalistic model for representing the presence of subpopulations, or clusters within an overall population or dataset in which the individual data points or observations are not required to have been ascribed to some subpopulation grouping or classification.

In simpler terms; a matrix that groups (unlabeled) datapoints together. Usually by closeness or similarity. It defines its own boundaries to try and bring some discernment to the raw data.

So, in this case, those boundaries are defined by some grouping or mixture of normal/Gaussian distributions. So they are soft, or probabalistic in nature.

This is a 1 dimensional Gaussian (bell curve).

![1D-Gaussian](/content/images/2018/04/1D-Gaussian.png)

Two components/dimensions.

![2dgauss](/content/images/2018/04/2dgauss.jpeg)

3D!

![4dgauss](/content/images/2018/06/4dgauss.jpg) 

But this one is probably more intuitive.

![ez4dgauss](/content/images/2018/06/ez4dgauss.jpg)

Where the probability increases as you get closer to the core or center, and we would combine as many of these as we would like cluster centers with which to describe the data.
We could as well use as many D's as we like, as long as we remember to connect each arbitrarily defined 'vertex', or degree or whatever to each other.
So if you could just imagine some spheroid connected to some inverted spheroid. And then one of those connected to an inverted one of those, and so on. Though, of course this is not the only way to imagine higher dimensional circles.

Here is the equation that describes the normal distribution. It is called the probability density function.

![pdf](/content/images/2018/04/pdf.svg)

This equation takes two parameters along with an input (x). The two parameters are the mean and standard deviation (though we may also use variance - standard deviation squared). The mean is the spot with the most area under the curve - so the high point in the single component bell curve graph pictured above, which has the greatest likelihood of being randomly selected from any point. The standard deviation measures the spread, or range of the curve. So a smaller standard deviation implies that greater amounts of the data lie closer to the mean.

Okay so we're drawing circles (ellipsoids) around data points, cool. How do we algorithmize this process? After all, a computer can't just look at these points,

![1Gdata](/content/images/2018/04/1Gdata.jpg)

and go 'Bam!'

![circleddata](/content/images/2018/04/circleddata.jpg)

Or sometimes the data could look like this

![data-1](/content/images/2018/04/data-1.jpg)

So we need some objective measure to calculate the probability of a point belonging to any given cluster.


###Expectation Maximization Algorithm

This equation is the expectation step.

![expectation](/content/images/2018/04/expectation.svg)

W is a matrix of probabilities that data point i is generated by 
Gaussian j.
Phi is our weights, representing the probability of choosing a Gaussian.
This is multiplied by the probability that a point (i) is from Gaussian (j).
This is all divided by the the summation of all weights and probabilities, in order to make the calculation a probability.



The first equation is the sum of all of the probabilities of Gaussian j,
divided by the number of points in the data set.

The second equation computes the mean of data points (i) times the probabilities for that cluster.

The third computes the covariance times the probabilities for that cluster.

And here is the basic Matlab/Octave code.
Check below this for the Expectation Maximization algorithm implementation. It is done for us within the fitgmdist() function in this code.

```matlab
%Generate normally distributed data
rng('default');
mu = [3 3];
mu2 = [1 2];
sigma = [0.6 0.4;0.4 0.5];
sigma2 = [0.5 0.2;0.2 0.5];
R1 = mvnrnd(mu,sigma, 25);
R2 = mvnrnd(mu2,sigma2, 25);

data = [R1; R2];
```

![mydata](/content/images/2018/04/mydata.jpg)

We can see that there is quite a bit of an overlap in our data. If there was not, simple K-means clustering would suffice with virtually 100 percent accuracy. But since there is, we add the metric of standard deviation to create a way to measure how likely it is for any point to belong to any cluster.

```
% This does EM for us
gm = fitgmdist(data,2);

% List of rgb values for each data point for plotting
myColors = zeros(size(data, 1), 3); 

for i=1:50
    if i<26
        myColors(i, :) = [0,0,1];
    else
        myColors(i, :) = [1,0,0];
    end
end

scatter(data(:,1), data(:,2), 10, myColors);
hold on
ezcontour(@(x,y)pdf(gm,[x y]),[-6 6],[-6 6])
```

And the resulting distributions.

![gmdata](/content/images/2018/04/gmdata.jpg)

The darker lines indicate lower scores. The scores are weighted log probabilities. Each point is generated by the weighted sum of Gaussians and we take the log for numerical stability, preventing underflow.
I will explain this further detail later or in another post.

Below here is the code that implements the previous four equations for expectation maximization iteratively. I will include a python implementation shortly.

So in short, a GMM finds the mean(s) and standard deviation or probability density as one strays from the mean, where the probability is 1 for each Gaussian distribution. Meaning that this model assumes that the data is in fact Gaussian, or close enough to it. Which the **central limit theorem** states that if enough data samples are collected, they will tend to resemble a Gaussian. Which means that this is an often useful clustering tool.


```matlab
% Generate dataset
mu = [3 3];
mu2 = [1 2];
sigma = [0.6 0.4;0.4 0.5];
sigma2 = [0.5 0.2;0.2 0.5];
R1 = mvnrnd(mu,sigma, 200);
R2 = mvnrnd(mu2,sigma2, 200);

data = [R1; R2];
%data = data';

[m, n] = size(data);
%n = size(data(2));
% Number of clusters
k = 2;

% Equal probability of each cluster for priors
phis = [0.5, 0.5];
% 'Random' means
mus = [0.3, 0.3; 3, 3];
% Calculate covariance matrix for dataset
sigmas = [];
for j = 1 : n
    sigmas{j} = cov(data);
end


for iter=1:1000
    disp(iter);
    %%===========================================================
    %Expectation step
    
    %Compute probability of each data point in each cluster
    probdensity = zeros(m, k);
    
    for j=1:k
        % The probability density function (Gaussian function)
        meanDiff = bsxfun(@minus, data, mus(j,:));
        
        probdensity(:,j) = 1 / sqrt((2*pi)^n * det(sigmas{j})) * exp(-1/2 * sum((meanDiff * inv(sigmas{j}) .* meanDiff), 2));
    end
    
    % Bottom half of division sign in the equation
    weights = bsxfun(@times, probdensity, phis);
    
    sumweights = sum(weights, 2);
    weights = bsxfun(@rdivide, weights, sumweights);
    
    %%===========================================================
    % Maximization step
    
    oldmeans = mus;
    for j=1:k
        % Find prior probability given weights
        % (The first equation in max-step)
        phis(j) = mean(weights(:,j),1);
        
        % Calculate weighted average for cluster
        % (Second equation)
        
        % Dot product of weights times data
        weightedavg = weights(:,j)' * data;
        
        mus(j,:) = weightedavg ./ sum(weights, 1);
        
        
        % (Third equation)
        sigma_k = zeros(n, n);
        
        % Data points minus cluster means
        recentereddata = bsxfun(@minus, data, mus(j,:));
        
        % Calculate how much each data element contributes to the
        % covariance
        
        for i=1:m
            sigma_k = sigma_k + (weights(i,j) .* (recentereddata(i,:)' * recentereddata(i,:)));
        end
        
        % Divide by sum of weights
        sigmas{j} = sigma_k ./ sum(weights(:, j));
    end
    % Check if converged
    if (sum(abs(mus - oldmeans)) < 0.000000000005)
       disp('Converged in ')
       disp(int2str(iter))
       disp('Iterations.')
            
       break
    end
    
    
end


% Plot result

% Display a scatter plot of the two distributions.
figure();
hold off;

plot(R1(:, 1), R1(:, 2), 'bo','MarkerSize',5);
hold on;
plot(R2(:, 1), R2(:, 2), 'ro','MarkerSize',5);
hold on;


plot(mu(1), mu(2), 'kx');
hold on;
plot(mu2(1), mu2(2), 'kx');
hold on;

% Create a [10,000 x 2] matrix 'gridX' of coordinates representing
% the input values over the grid.
gridSize = 100;
u = linspace(-6, 6, gridSize);
[A, B] = meshgrid(u, u);
gridX = [A(:), B(:)];

% Calculate the Gaussian response for every value in the grid.
n = size(gridX,2);
meanDiff = bsxfun(@minus, gridX, mus(1,:));
pdf1 = 1 / sqrt((2*pi)^n * det(sigmas{1})) * exp(-1/2 * sum((meanDiff * inv(sigmas{1}) .* meanDiff), 2));

meanDiff = bsxfun(@minus, gridX, mus(2,:));
pdf2 = 1 / sqrt((2*pi)^n * det(sigmas{2})) * exp(-1/2 * sum((meanDiff * inv(sigmas{2}) .* meanDiff), 2));

% Reshape the responses back into a 2D grid to be plotted with contour.
Z1 = reshape(pdf1, gridSize, gridSize);
Z2 = reshape(pdf2, gridSize, gridSize);

% Plot the contour lines to show the pdf over the data.
[C, h] = contour(u, u, Z1);
[C, h] = contour(u, u, Z2);
axis([-3 6 0 6])

title('Training Data and Estimated PDFs');

```
